{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForTokenClassification, BertConfig, IntervalStrategy, TrainingArguments, Trainer\n",
    "import datasets\n",
    "from datasets.arrow_dataset import Dataset\n",
    "import ruamel.yaml\n",
    "yaml = ruamel.yaml.YAML()\n",
    "\n",
    "import abctk.obj.comparative as aoc\n",
    "import abct_comp_ner_utils.models.NER_with_root as nwr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-60f4c3a656674579\n",
      "Found cached dataset parquet (/home/twotrees12/.cache/huggingface/datasets/abctreebank___parquet/default-60f4c3a656674579/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b161a794c1b5421eadbd2ea4ab291b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_raw = datasets.load_dataset(\n",
    "    \"abctreebank/comparative-NER-BCCWJ\",\n",
    "    use_auth_token = True,\n",
    "    revision = \"18dcd7235a4ae43a3517b0545314c888a579995e\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MODEL_PATH = \"../results_2022-12-27/\"\n",
    "\n",
    "tokenizer = nwr.get_tokenizer()\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/twotrees12/.cache/huggingface/datasets/abctreebank___parquet/default-60f4c3a656674579/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8a2e51971ad84ef9.arrow\n"
     ]
    }
   ],
   "source": [
    "ds_test = dataset_raw[\"test\"].map(\n",
    "    lambda E: nwr.convert_annotation_entries_to_matrices(\n",
    "        E,\n",
    "        return_type = \"pt\",\n",
    "    ),\n",
    "    batched = True,\n",
    "    batch_size = BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/twotrees12/.cache/huggingface/datasets/abctreebank___parquet/default-60f4c3a656674579/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d9e3852ed0114d36.arrow\n"
     ]
    }
   ],
   "source": [
    "def _predict(\n",
    "    examples: datasets.arrow_dataset.Batch\n",
    "):\n",
    "    predictions_raw = model.forward(\n",
    "        input_ids = torch.tensor(examples[\"input_ids\"]).cuda(),\n",
    "        attention_mask = torch.tensor(examples[\"attention_mask\"]).cuda(),\n",
    "        token_type_ids  = torch.tensor(examples[\"token_type_ids\"]).cuda(),\n",
    "        return_dict = True,\n",
    "    )\n",
    "\n",
    "    examples[\"label_ids_predicted_NER\"] = (\n",
    "        predictions_raw.logits\n",
    "        .argmax(dim = 2,)\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    return examples\n",
    "# === END ===\n",
    "\n",
    "ds_test = ds_test.map(\n",
    "    lambda e: (\n",
    "        nwr.convert_predictions_to_annotations(\n",
    "            nwr.convert_predictions_to_annotations(\n",
    "                _predict(e),\n",
    "                label_ids_key = \"label_ids_predicted_NER\",\n",
    "                comp_key = \"comp_predicted_NER\",\n",
    "            ),\n",
    "            label_ids_key = \"label_ids\",\n",
    "            comp_key = \"comp_subword_aligned\",\n",
    "        )\n",
    "    ),\n",
    "    batched = True,\n",
    "    batch_size = BATCH_SIZE,\n",
    ")\n",
    "metric = nwr.NERWithRootMetrics()\n",
    "metric.add_batch(\n",
    "    predictions = ds_test[\"label_ids_predicted_NER\"],\n",
    "    references = ds_test[\"label_ids\"],\n",
    ")\n",
    "metric_NER = metric.compute()\n",
    "ds_test = ds_test.add_column(\n",
    "    \"matching_NER\",\n",
    "    [\n",
    "        aoc.print_AlignResult(\n",
    "            prediction = pred,\n",
    "            reference = ref,\n",
    "            alignment = align\n",
    "        )\n",
    "        for pred, ref, align in zip(\n",
    "            ds_test[\"comp_predicted_NER\"],\n",
    "            ds_test[\"comp_subword_aligned\"],\n",
    "            metric_NER[\"alignments\"],\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ref: (9-18)root \\t ↔ pred: (9-18)root',\n",
       " 'ref: (9-12)cont \\t ↔ pred: (9-12)cont',\n",
       " 'ref: (12-14)prej \\t ↔ pred: (12-14)prej',\n",
       " 'ref: (14-15)diff \\t ↔ pred: (14-15)diff',\n",
       " 'ref: (15-17)deg \\t ↔ pred: (15-17)deg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test[\"matching_NER\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "0.7047913446676971\n",
      "0.5007727975270478\n",
      "\n",
      "cont\n",
      "0.7167487684729065\n",
      "0.5763546798029557\n",
      "\n",
      "prej\n",
      "0.8927943760984183\n",
      "0.8471001757469245\n",
      "\n",
      "diff\n",
      "0.8390804597701149\n",
      "0.7701149425287357\n",
      "\n",
      "deg\n",
      "0.8242320819112628\n",
      "0.7610921501706485\n",
      "\n",
      "0.79552940618408 0.6910869491552625\n"
     ]
    }
   ],
   "source": [
    "for label, res in metric_NER[\"scores_spanwise\"].items():\n",
    "    print(label)\n",
    "    print(res[\"F1_partial\"])\n",
    "    print(res[\"F1_strict\"])\n",
    "    print()\n",
    "\n",
    "print(metric_NER[\"F1_partial_average\"], metric_NER[\"F1_strict_average\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based model based on Ginza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300f10318d8d4394aa13ac078c2d3cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/350 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"./test_spacy.jsonl\") as f:\n",
    "    ds_rule_predicted= dict(\n",
    "        (record[\"ID\"], record)\n",
    "        for record in map(\n",
    "            lambda c: aoc.dice_CompRecord(**json.loads(c)),\n",
    "            filter(None, map(str.strip, f))\n",
    "        )\n",
    "    )\n",
    "\n",
    "def _add_rulebased_prediction(\n",
    "    entry, \n",
    "    preds: dict[str, aoc.CompRecord] = ds_rule_predicted\n",
    "):\n",
    "    ID = entry[\"ID\"]\n",
    "    diced = aoc.dice_CompRecord(\n",
    "        tokens = entry[\"tokens\"], comp = entry[\"comp\"],\n",
    "        ID = ID, \n",
    "    )\n",
    "    comp_diced = []\n",
    "    for span in diced[\"comp\"]:\n",
    "        label = span[\"label\"]\n",
    "        if (match := nwr._RE_FEAT_ARTIFACTS.match(label) ):\n",
    "            label = match.group(\"name\") or label\n",
    "        \n",
    "        comp_diced.append(\n",
    "            {\n",
    "                \"start\": span[\"start\"],\n",
    "                \"end\": span[\"end\"],\n",
    "                \"label\": label,\n",
    "            }\n",
    "        )\n",
    "    entry[\"tokens_diced\"] = diced[\"tokens\"]\n",
    "    entry[\"comp_diced\"] = comp_diced\n",
    "    \n",
    "    entry[\"comp_predicted_rulebased\"] = preds[ID][\"comp\"] if ID in preds else None\n",
    "\n",
    "    return entry\n",
    "\n",
    "ds_test = ds_test.map(_add_rulebased_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_rulebased = aoc.calc_prediction_metrics(\n",
    "    predictions = (\n",
    "        rec[\"comp_predicted_rulebased\"] for rec in ds_test\n",
    "        if rec[\"comp_predicted_rulebased\"] is not None\n",
    "    ),\n",
    "    references = (\n",
    "        rec[\"comp_diced\"] for rec in ds_test\n",
    "        if rec[\"comp_predicted_rulebased\"] is not None\n",
    "    )\n",
    ")\n",
    "\n",
    "ds_test = ds_test.add_column(\n",
    "    \"matching_rulebased\",\n",
    "    [\n",
    "        aoc.print_AlignResult(\n",
    "            prediction = pred,\n",
    "            reference = ref,\n",
    "            alignment = align\n",
    "        )\n",
    "        for pred, ref, align in zip(\n",
    "            ds_test[\"comp_predicted_rulebased\"],\n",
    "            ds_test[\"comp_diced\"],\n",
    "            metric_rulebased[\"alignments\"],\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prej\n",
      "0.8441971383147854\n",
      "0.8139904610492845\n",
      "\n",
      "deg\n",
      "0.6911076443057722\n",
      "0.5522620904836194\n",
      "\n",
      "cont\n",
      "0.3959731543624161\n",
      "0.1375838926174497\n",
      "\n",
      "diff\n",
      "0.483050847457627\n",
      "0.3983050847457627\n",
      "\n",
      "root\n",
      "nan\n",
      "nan\n",
      "\n",
      "0.6035821961101502 0.47553538222402914\n"
     ]
    }
   ],
   "source": [
    "for label, res in metric_rulebased[\"scores_spanwise\"].items():\n",
    "    print(label)\n",
    "    print(res[\"F1_partial\"])\n",
    "    print(res[\"F1_strict\"])\n",
    "    print()\n",
    "\n",
    "print(metric_rulebased[\"F1_partial_average\"], metric_rulebased[\"F1_strict_average\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rulebased_scores.yaml\", \"w\") as f:\n",
    "    yaml.dump(\n",
    "        {\n",
    "            k: v for k, v in metric_rulebased.items()\n",
    "            if k != \"alignments\"\n",
    "        },\n",
    "        stream = f\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./details.yaml\", \"w\") as g:\n",
    "    yaml.dump(\n",
    "        list(\n",
    "            {\n",
    "                \"ID\": entry[\"ID\"],\n",
    "                \"reference_linearlized\": aoc.linearize_annotations(\n",
    "                    tokens = entry[\"tokens\"],\n",
    "                    comp = entry[\"comp\"]\n",
    "                ),\n",
    "                \"predicted_NER_linearized\": aoc.linearize_annotations(\n",
    "                    tokens = entry[\"token_subwords\"],\n",
    "                    comp = entry[\"comp_predicted_NER\"],\n",
    "                ),\n",
    "                \"matching_NER\": entry[\"matching_NER\"],\n",
    "                \"predicted_rulebased_linearized\": aoc.linearize_annotations(\n",
    "                    tokens = entry[\"tokens_diced\"],\n",
    "                    comp = entry[\"comp_predicted_rulebased\"],\n",
    "                ),\n",
    "                \"matching_rulebased\": entry[\"matching_rulebased\"],\n",
    "            }\n",
    "            for entry in ds_test\n",
    "        ),\n",
    "        stream = g,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comparative-ner-utils-TwdyRrPe-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1fa0ffdb9e9fd0b877c4ec03dcf1edf281110581bbc6c24c1d31e34cc50dc31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
